{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a564418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "\n",
    "# --- 用户配置 ---\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "save_dir = \"E:\"\n",
    "platforms = ['dy', 'xhs', 'tieba']\n",
    "comment_column = 'content'\n",
    "sentiment_column = 'sentiment'\n",
    "start_date_str = '2024-02-01'\n",
    "end_date_str = '2025-3-31'\n",
    "agg_freq = '2D'\n",
    "output_plot_filename = os.path.join(save_dir, \"sentiment_evolution_plot.svg\")\n",
    "\n",
    "# --- 字体设置 ---\n",
    "try:\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "except:\n",
    "    try:\n",
    "        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "    except:\n",
    "        print(\"警告：未找到中文字体。\")\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# --- 情感分数映射 ---\n",
    "sentiment_mapping = {\n",
    "    'Very Positive': 0.5,\n",
    "    'Positive': 0.25,\n",
    "    'Neutral': 0,\n",
    "    'Negative': -0.25,\n",
    "    'Very Negative': -0.5\n",
    "}\n",
    "\n",
    "# --- 平台配置 ---\n",
    "platform_configs = {\n",
    "    'dy':    {'time_col': 'create_time', 'like_col': 'like_count'},\n",
    "    'xhs':   {'time_col': 'time', 'like_col': 'like_count'},\n",
    "    'tieba': {'time_col': 'publish_time', 'like_col': None}\n",
    "}\n",
    "\n",
    "# --- 时间范围 ---\n",
    "try:\n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str)\n",
    "except ValueError:\n",
    "    print(f\"错误：开始或结束日期格式不正确。\")\n",
    "    exit()\n",
    "\n",
    "all_platform_sentiment = {}\n",
    "\n",
    "# --- 数据处理 ---\n",
    "print(\"开始处理数据...\")\n",
    "for platform in platforms:\n",
    "    print(f\"--- 处理平台: {platform} ---\")\n",
    "    config = platform_configs[platform]\n",
    "    time_col = config['time_col']\n",
    "    like_col = config['like_col']\n",
    "    file_path = os.path.join(save_dir, f\"{platform}_combined.csv\")\n",
    "    print(f\"读取文件: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        use_cols = [time_col, sentiment_column]\n",
    "        if like_col:\n",
    "            use_cols.append(like_col)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"警告: 文件未找到 {file_path}。跳过。\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(file_path, usecols=use_cols, low_memory=False)\n",
    "        print(f\"原始数据行数: {len(df)}\")\n",
    "\n",
    "        # --- 时间列清洗 ---\n",
    "        print(f\"处理时间列: {time_col}\")\n",
    "        if not pd.api.types.is_string_dtype(df[time_col]) and not pd.api.types.is_object_dtype(df[time_col]):\n",
    "            df[time_col] = df[time_col].astype(str)\n",
    "        df[time_col] = df[time_col].fillna('')\n",
    "        df[time_col] = df[time_col].str.strip()\n",
    "        df[time_col] = df[time_col].str.replace(r'\\s+', ' ', regex=True)\n",
    "        df[time_col] = df[time_col].str.strip()\n",
    "        cleaned_time_col = df[time_col].copy()\n",
    "\n",
    "        # --- 时间格式转换 ---\n",
    "        datetime_format = None\n",
    "        try:\n",
    "            if platform == 'dy':\n",
    "                datetime_format = '%Y-%m-%d %H:%M:%S'\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, format=datetime_format, errors='coerce')\n",
    "            elif platform == 'xhs':\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, errors='coerce')\n",
    "            else:\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, errors='coerce')\n",
    "        except ValueError as e:\n",
    "            print(f\"解析时间错误: {e}\")\n",
    "            df[time_col] = pd.NaT\n",
    "\n",
    "        invalid_time_mask = df[time_col].isna()\n",
    "        num_total_invalid = invalid_time_mask.sum()\n",
    "        print(f\"无效时间值: {num_total_invalid}\")\n",
    "\n",
    "        df.dropna(subset=[time_col], inplace=True)\n",
    "        print(f\"有效时间数据行数: {len(df)}\")\n",
    "\n",
    "        if not df.empty:\n",
    "            df[time_col] = df[time_col].dt.normalize()\n",
    "        else:\n",
    "            print(f\"警告：平台 {platform} 时间清洗后无数据。\")\n",
    "\n",
    "        # --- 情感分数处理 ---\n",
    "        print(f\"处理情感列: {sentiment_column}\")\n",
    "        df['sentiment_score'] = df[sentiment_column].map(sentiment_mapping)\n",
    "        df.dropna(subset=['sentiment_score'], inplace=True)\n",
    "        if df.empty:\n",
    "            print(f\"警告: 平台 {platform} 映射情感分数后无数据。\")\n",
    "            continue\n",
    "\n",
    "        # --- 加权情感分数 ---\n",
    "        print(\"计算加权情感分数...\")\n",
    "        if like_col:\n",
    "            df[like_col] = pd.to_numeric(df[like_col], errors='coerce').fillna(0)\n",
    "            df[like_col] = df[like_col].clip(lower=0)\n",
    "            df['weighted_sentiment'] = df['sentiment_score'] * np.log1p(df[like_col])\n",
    "            df['weighted_sentiment'] = df['weighted_sentiment'].clip(lower=-0.35, upper=0.35)\n",
    "        else:\n",
    "            df['weighted_sentiment'] = df['sentiment_score'].clip(lower=-0.35, upper=0.35)\n",
    "\n",
    "        # --- 按时间聚合 ---\n",
    "        print(f\"按时间聚合 (频率: {agg_freq})...\")\n",
    "        if df.empty or 'weighted_sentiment' not in df.columns:\n",
    "            print(f\"警告: 平台 {platform} 无有效加权情感数据。\")\n",
    "            continue\n",
    "\n",
    "        df.set_index(time_col, inplace=True)\n",
    "        sentiment_ts = df['weighted_sentiment'].resample(agg_freq).mean()\n",
    "        if sentiment_ts.empty:\n",
    "            print(f\"警告: 平台 {platform} 重采样后结果为空。\")\n",
    "            continue\n",
    "\n",
    "        sentiment_ts.ffill(inplace=True)\n",
    "        sentiment_ts.bfill(inplace=True)\n",
    "        if sentiment_ts.isna().all():\n",
    "            print(f\"警告: 平台 {platform} 聚合后全部为 NaN。\")\n",
    "            continue\n",
    "\n",
    "        all_platform_sentiment[platform] = sentiment_ts\n",
    "        print(f\"平台 {platform} 处理完成。聚合后时间点: {len(sentiment_ts)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"错误: 列名错误 - {e}。\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 处理平台 {platform} 时发生错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# --- 绘图 ---\n",
    "print(\"\\n开始绘制情感演变图...\")\n",
    "if not all_platform_sentiment:\n",
    "    print(\"错误：无有效数据，无法绘图。\")\n",
    "else:\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    colors = ['#1f77b4', '#ff7f0e', 'green']\n",
    "    color_map = {platform: colors[i] for i, platform in enumerate(all_platform_sentiment.keys())}\n",
    "\n",
    "    for platform, sentiment_ts in all_platform_sentiment.items():\n",
    "        if not sentiment_ts.empty:\n",
    "            rolling_mean_ts = sentiment_ts.rolling(window=7, center=True, min_periods=1).mean()\n",
    "            plt.plot(rolling_mean_ts.index, rolling_mean_ts.values,\n",
    "                     label=f\"{platform.upper()} 平台\",\n",
    "                     color=color_map[platform],\n",
    "                     linewidth=2,\n",
    "                     linestyle='-')\n",
    "        else:\n",
    "            print(f\"警告: 平台 {platform} 无聚合数据可绘制。\")\n",
    "\n",
    "    plt.xlim(start_date, end_date)\n",
    "    plt.title(f'各平台对国产大模型情感演变趋势', fontsize=18, pad=20)\n",
    "    plt.xlabel('日期', fontsize=14, labelpad=10)\n",
    "    plt.ylabel('平均加权情感得分', fontsize=14, labelpad=10)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "    if agg_freq == 'D':\n",
    "        total_days = (end_date - start_date).days\n",
    "        if total_days > 180:\n",
    "            plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "            plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "        elif total_days > 60:\n",
    "            plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "            plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "        else:\n",
    "            plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "            plt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n",
    "    elif agg_freq == 'W':\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "    else:\n",
    "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(minticks=8, maxticks=15))\n",
    "\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8, alpha=0.7)\n",
    "    plt.legend(title=\"平台\", fontsize=12, title_fontsize=13, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "    try:\n",
    "        plt.savefig(output_plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"图像已保存至: {output_plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 保存图像失败 - {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n处理完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "import math\n",
    "import traceback \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 用户配置 ---\n",
    "try:\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "except:\n",
    "    try:\n",
    "        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "    except:\n",
    "        print(\"警告：未找到中文字体。\")\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "save_dir = \"E\"\n",
    "platforms = ['dy', 'xhs', 'tieba']\n",
    "sentiment_column = 'sentiment'\n",
    "start_date_str = '2024-02-01'\n",
    "end_date_str = '2025-04-01'\n",
    "agg_freq = '2D'\n",
    "forecast_months = 2\n",
    "forecast_output_dir = os.path.join(save_dir, f\"forecast_results_transformer_lstm_{forecast_months}mo\")\n",
    "time_step = 15\n",
    "hidden_size = 64\n",
    "encoder_layers = 3\n",
    "lstm_layers = 1\n",
    "dropout = 0.1\n",
    "epochs = 60\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if not os.path.exists(forecast_output_dir):\n",
    "    os.makedirs(forecast_output_dir)\n",
    "    print(f\"已创建目录: {forecast_output_dir}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"\\n--- 开始加载和预处理数据 ---\")\n",
    "\n",
    "sentiment_mapping = {\n",
    "    'Very Positive': 0.5,\n",
    "    'Positive': 0.25,\n",
    "    'Neutral': 0,\n",
    "    'Negative': -0.25,\n",
    "    'Very Negative': -0.5\n",
    "}\n",
    "\n",
    "platform_configs = {\n",
    "    'dy':    {'time_col': 'create_time', 'like_col': 'like_count'},\n",
    "    'xhs':   {'time_col': 'time', 'like_col': 'like_count'},\n",
    "    'tieba': {'time_col': 'publish_time', 'like_col': None}\n",
    "}\n",
    "\n",
    "try:\n",
    "    start_date_filter = pd.to_datetime(start_date_str)\n",
    "    end_date_filter = pd.to_datetime(end_date_str)\n",
    "    print(f\"原始数据过滤范围: {start_date_filter.date()} 至 {end_date_filter.date()}\")\n",
    "except ValueError:\n",
    "    print(f\"错误：日期格式不正确。\")\n",
    "    exit()\n",
    "\n",
    "all_platform_sentiment = {}\n",
    "\n",
    "for platform in platforms:\n",
    "    print(f\"\\n--- 处理平台: {platform} ---\")\n",
    "    config = platform_configs[platform]\n",
    "    time_col = config['time_col']\n",
    "    like_col = config['like_col']\n",
    "    file_path = os.path.join(save_dir, f\"{platform}_combined.csv\")\n",
    "    print(f\"读取文件: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        use_cols = [time_col, sentiment_column]\n",
    "        if like_col:\n",
    "            use_cols.append(like_col)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"警告: 文件未找到 {file_path}。跳过。\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(file_path, usecols=use_cols, low_memory=False)\n",
    "        print(f\"原始数据行数: {len(df)}\")\n",
    "\n",
    "        if not pd.api.types.is_string_dtype(df[time_col]) and not pd.api.types.is_object_dtype(df[time_col]):\n",
    "            df[time_col] = df[time_col].astype(str)\n",
    "        df[time_col] = df[time_col].fillna('')\n",
    "        try:\n",
    "            df[time_col] = df[time_col].str.strip()\n",
    "            if pd.api.types.is_string_dtype(df[time_col]):\n",
    "                df[time_col] = df[time_col].str.replace(r'\\s+', ' ', regex=True)\n",
    "                df[time_col] = df[time_col].str.strip()\n",
    "        except AttributeError:\n",
    "            print(f\"警告: 无法清理时间列。\")\n",
    "\n",
    "        cleaned_time_col = df[time_col].copy()\n",
    "        try:\n",
    "            if platform == 'dy':\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "            elif platform == 'xhs':\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, errors='coerce')\n",
    "            else:\n",
    "                df[time_col] = pd.to_datetime(cleaned_time_col, errors='coerce')\n",
    "        except ValueError as e:\n",
    "            print(f\"错误: 时间解析失败: {e}\")\n",
    "            df[time_col] = pd.NaT\n",
    "\n",
    "        invalid_time_mask = df[time_col].isna()\n",
    "        num_total_invalid = invalid_time_mask.sum()\n",
    "        if num_total_invalid > 0:\n",
    "            print(f\"无效时间值: {num_total_invalid}\")\n",
    "\n",
    "        original_rows = len(df)\n",
    "        df.dropna(subset=[time_col], inplace=True)\n",
    "        removed_count = original_rows - len(df)\n",
    "        if removed_count > 0:\n",
    "            print(f\"删除无效时间行: {removed_count}\")\n",
    "        print(f\"有效时间数据行: {len(df)}\")\n",
    "        if df.empty:\n",
    "            print(f\"警告: 平台 {platform} 时间清洗后无数据。\")\n",
    "            continue\n",
    "\n",
    "        df = df[(df[time_col] >= start_date_filter) & (df[time_col] <= end_date_filter)]\n",
    "        print(f\"日期范围内数据行: {len(df)}\")\n",
    "        if df.empty:\n",
    "            print(f\"警告: 平台 {platform} 日期范围内无数据。\")\n",
    "            continue\n",
    "\n",
    "        df['sentiment_score'] = df[sentiment_column].map(sentiment_mapping)\n",
    "        original_rows = len(df)\n",
    "        df.dropna(subset=['sentiment_score'], inplace=True)\n",
    "        removed_count = original_rows - len(df)\n",
    "        print(f\"情感有效数据行: {len(df)}\")\n",
    "        if df.empty:\n",
    "            print(f\"警告: 平台 {platform} 映射情感分数后无数据。\")\n",
    "            continue\n",
    "\n",
    "        if like_col:\n",
    "            df[like_col] = pd.to_numeric(df[like_col], errors='coerce')\n",
    "            df[like_col] = df[like_col].fillna(0)\n",
    "            neg_likes_mask = df[like_col] < 0\n",
    "            if neg_likes_mask.any():\n",
    "                df.loc[neg_likes_mask, like_col] = 0\n",
    "            df['weight'] = np.log1p(df[like_col])\n",
    "            df['weighted_sum_component'] = df['sentiment_score'] * df['weight']\n",
    "        else:\n",
    "            df['weight'] = 1.0\n",
    "            df['weighted_sum_component'] = df['sentiment_score'] * df['weight']\n",
    "\n",
    "        print(f\"按时间聚合 (频率: {agg_freq})...\")\n",
    "        if df.empty or 'weighted_sum_component' not in df.columns or 'weight' not in df.columns:\n",
    "            print(f\"警告: 平台 {platform} 无有效加权情感数据。\")\n",
    "            continue\n",
    "\n",
    "        if df.index.name != time_col:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "                print(f\"警告: 时间列类型不对，尝试转换。\")\n",
    "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
    "                df.dropna(subset=[time_col], inplace=True)\n",
    "            if df.empty:\n",
    "                print(f\"警告: 平台 {platform} 时间列转换后无数据。\")\n",
    "                continue\n",
    "            df.set_index(time_col, inplace=True)\n",
    "\n",
    "        resampled_data = df.resample(agg_freq).agg(\n",
    "            weighted_sum=('weighted_sum_component', 'sum'),\n",
    "            total_weight=('weight', 'sum')\n",
    "        )\n",
    "\n",
    "        sentiment_ts = resampled_data['weighted_sum'].copy()\n",
    "        non_zero_weight_mask = resampled_data['total_weight'] != 0\n",
    "        sentiment_ts.loc[non_zero_weight_mask] /= resampled_data.loc[non_zero_weight_mask, 'total_weight']\n",
    "        sentiment_ts.loc[~non_zero_weight_mask] = np.nan\n",
    "\n",
    "        sentiment_ts.ffill(inplace=True)\n",
    "        sentiment_ts.bfill(inplace=True)\n",
    "        final_nan_count = sentiment_ts.isna().sum()\n",
    "        if final_nan_count > 0:\n",
    "            print(f\"警告: 填充后仍有 NaN: {final_nan_count}\")\n",
    "\n",
    "        sentiment_ts = sentiment_ts.clip(lower=-0.35, upper=0.35)\n",
    "\n",
    "        if sentiment_ts.isna().all() or sentiment_ts.empty:\n",
    "            print(f\"警告: 平台 {platform} 聚合后无有效数据。\")\n",
    "            continue\n",
    "\n",
    "        all_platform_sentiment[platform] = sentiment_ts\n",
    "        print(f\"平台 {platform} 处理完成。聚合时间点: {len(sentiment_ts)}\")\n",
    "        print(f\"时间范围: {sentiment_ts.index.min().date()} 至 {sentiment_ts.index.max().date()}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"错误: 列名错误 - {e}\")\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 发生意外错误: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.seq_length)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_seq = self.data[index : index + self.seq_length]\n",
    "        target_val = self.data[index + self.seq_length]\n",
    "        return torch.tensor(input_seq, dtype=torch.float32), torch.tensor(target_val, dtype=torch.float32)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 != 0:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos_encoding = self.pe[:x.size(0), :].unsqueeze(1)\n",
    "        return x + pos_encoding.to(x.device)\n",
    "\n",
    "class TransformerLSTM(nn.Module):\n",
    "    def __init__(self, input_feature_size=1, hidden_size=64, num_encoder_layers=3, num_lstm_layers=1, dropout=0.1, nhead=8):\n",
    "        super(TransformerLSTM, self).__init__()\n",
    "        self.input_linear = nn.Linear(input_feature_size, hidden_size)\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, max_len=time_step + 50)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead, dim_feedforward=hidden_size*4, dropout=dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_lstm_layers, dropout=dropout if num_lstm_layers > 1 else 0, batch_first=False)\n",
    "        self.output_linear = nn.Linear(hidden_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.output_linear.bias.data.zero_()\n",
    "        self.output_linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.input_linear(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        mask = self._generate_square_subsequent_mask(src.size(0)).to(src.device)\n",
    "        transformer_output = self.transformer_encoder(src, mask=mask)\n",
    "        lstm_output, (hn, cn) = self.lstm(transformer_output)\n",
    "        last_hidden_state = hn[-1, :, :]\n",
    "        prediction = self.output_linear(last_hidden_state)\n",
    "        return prediction\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, epoch, total_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"训练 Epoch {epoch+1}/{total_epochs}\", leave=False)\n",
    "    for batch_idx, (input_seq, target_val) in enumerate(pbar):\n",
    "        input_seq = input_seq.unsqueeze(-1).to(device)\n",
    "        target_val = target_val.unsqueeze(-1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output, target_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'批次损失': f'{loss.item():.4f}'})\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def predict_future(model, initial_sequence, steps, scaler, device):\n",
    "    model.eval()\n",
    "    predictions_scaled = []\n",
    "    current_sequence = initial_sequence.clone()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            input_tensor = current_sequence.unsqueeze(0).unsqueeze(-1).to(device)\n",
    "            next_pred_scaled = model(input_tensor)\n",
    "            pred_val = next_pred_scaled.item()\n",
    "            predictions_scaled.append(pred_val)\n",
    "            new_sequence_member = torch.tensor([pred_val], dtype=torch.float32, device=device)\n",
    "            current_sequence = torch.cat((current_sequence[1:], new_sequence_member), dim=0)\n",
    "    predictions_scaled_np = np.array(predictions_scaled).reshape(-1, 1)\n",
    "    predictions_unscaled = scaler.inverse_transform(predictions_scaled_np)\n",
    "    return predictions_unscaled.flatten()\n",
    "\n",
    "all_platform_forecasts = {}\n",
    "all_platform_scalers = {}\n",
    "all_platform_histories = {}\n",
    "\n",
    "print(f\"\\n--- 开始训练并预测 ---\")\n",
    "print(f\"--- 使用设备: {device} ---\")\n",
    "\n",
    "if not all_platform_sentiment:\n",
    "    print(\"\\n错误：无数据，无法训练和预测。\")\n",
    "    exit()\n",
    "\n",
    "platform_pbar = tqdm(all_platform_sentiment.items(), desc=\"处理平台\")\n",
    "for platform, ts_data in platform_pbar:\n",
    "    platform_pbar.set_postfix({'当前平台': platform.upper()})\n",
    "    if len(ts_data) < time_step + 1:\n",
    "        print(f\"平台 {platform} 数据量不足，跳过。\")\n",
    "        continue\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_scaled = scaler.fit_transform(ts_data.values.reshape(-1, 1)).flatten()\n",
    "    all_platform_scalers[platform] = scaler\n",
    "    all_platform_histories[platform] = ts_data\n",
    "\n",
    "    full_dataset = SentimentDataset(data_scaled, time_step)\n",
    "    if len(full_dataset) == 0:\n",
    "        print(f\"警告: 平台 {platform} 数据量不足，跳过。\")\n",
    "        continue\n",
    "\n",
    "    train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    print(f\"平台 {platform}: 训练样本数 {len(full_dataset)}\")\n",
    "\n",
    "    model = TransformerLSTM(input_feature_size=1,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_encoder_layers=encoder_layers,\n",
    "                            num_lstm_layers=lstm_layers,\n",
    "                            dropout=dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"开始训练 {platform} ({epochs} epochs)...\")\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = train_model(model, train_loader, optimizer, criterion, device, epoch, epochs)\n",
    "        train_losses.append(epoch_loss)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"{platform} - Epoch {epoch+1}/{epochs}, 损失: {epoch_loss:.6f}\")\n",
    "\n",
    "    print(f\"模型训练完成，开始预测 {platform} ...\")\n",
    "    last_sequence_scaled = data_scaled[-time_step:]\n",
    "    initial_input_tensor = torch.tensor(last_sequence_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "    last_date = ts_data.index[-1]\n",
    "    forecast_end_date = last_date + pd.DateOffset(months=forecast_months)\n",
    "    next_pred_start_date = last_date + pd.Timedelta(agg_freq)\n",
    "    future_index = pd.date_range(start=next_pred_start_date, end=forecast_end_date, freq=agg_freq)\n",
    "    num_forecast_steps = len(future_index)\n",
    "\n",
    "    if num_forecast_steps <= 0:\n",
    "        print(f\"警告: 平台 {platform} 预测步数为 0，跳过。\")\n",
    "        continue\n",
    "\n",
    "    print(f\"预测未来 {num_forecast_steps} 步 ({agg_freq}), {future_index[0].date()} 到 {future_index[-1].date()}...\")\n",
    "\n",
    "    forecast_values_unscaled = predict_future(model, initial_input_tensor, num_forecast_steps, scaler, device)\n",
    "    forecast_series = pd.Series(forecast_values_unscaled, index=future_index)\n",
    "    all_platform_forecasts[platform] = forecast_series\n",
    "    print(f\"平台 {platform} 预测完成。\")\n",
    "\n",
    "print(\"\\n--- 绘制预测图 ---\")\n",
    "\n",
    "if not all_platform_forecasts and not all_platform_histories:\n",
    "    print(\"错误：无历史或预测结果可绘制。\")\n",
    "else:\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    platform_keys = list(all_platform_histories.keys())\n",
    "    platform_color_map = {platform: colors[i % len(colors)] for i, platform in enumerate(platform_keys)}\n",
    "\n",
    "    print(\"绘制历史数据...\")\n",
    "    plotted_history = False\n",
    "    for platform, history_ts in all_platform_histories.items():\n",
    "        if not history_ts.empty:\n",
    "            plt.plot(history_ts.index, history_ts.values,\n",
    "                     label=f'{platform.upper()} 历史情感',\n",
    "                     color=platform_color_map.get(platform, 'gray'),\n",
    "                     linewidth=1.5, alpha=0.8)\n",
    "            plotted_history = True\n",
    "        else:\n",
    "            print(f\"警告: 平台 {platform} 无历史数据。\")\n",
    "    if not plotted_history: print(\"警告：无历史数据被绘制。\")\n",
    "\n",
    "    print(\"绘制预测数据...\")\n",
    "    plot_end_date = None\n",
    "    plotted_forecast = False\n",
    "    for platform, forecast_ts in all_platform_forecasts.items():\n",
    "        if not forecast_ts.empty:\n",
    "            plt.plot(forecast_ts.index, forecast_ts.values,\n",
    "                     label=f'{platform.upper()} 预测情感',\n",
    "                     color=platform_color_map.get(platform, 'black'),\n",
    "                     linestyle='--', marker='o', markersize=4, linewidth=2.0)\n",
    "            current_max_date = forecast_ts.index[-1]\n",
    "            if plot_end_date is None or current_max_date > plot_end_date:\n",
    "                plot_end_date = current_max_date\n",
    "            plotted_forecast = True\n",
    "        else:\n",
    "            if platform in all_platform_histories:\n",
    "                print(f\"警告: 平台 {platform} 无预测数据。\")\n",
    "    if not plotted_forecast: print(\"警告：无预测数据被绘制。\")\n",
    "\n",
    "    plt.title(f'各平台情感演变与未来 {forecast_months} 个月预测 (Transformer-LSTM)', fontsize=18, pad=20)\n",
    "    plt.xlabel('日期', fontsize=14, labelpad=10)\n",
    "    plt.ylabel('平均加权情感得分', fontsize=14, labelpad=10)\n",
    "    plt.axhline(0, color='grey', linestyle=':', linewidth=1.0, alpha=0.8)\n",
    "\n",
    "    try:\n",
    "        plot_start_date = min(ts.index[0] for ts in all_platform_histories.values() if not ts.empty)\n",
    "    except ValueError:\n",
    "        plot_start_date = pd.to_datetime(start_date_str)\n",
    "\n",
    "    if plot_end_date is None:\n",
    "        try:\n",
    "            plot_end_date = max(ts.index[-1] for ts in all_platform_histories.values() if not ts.empty)\n",
    "        except ValueError:\n",
    "            plot_end_date = pd.to_datetime(end_date_str)\n",
    "    plot_end_date += timedelta(days=5)\n",
    "\n",
    "    plt.xlim(plot_start_date, plot_end_date)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(minticks=10, maxticks=20))\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.legend(title=\"平台\", fontsize=12, title_fontsize=13, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 1])\n",
    "\n",
    "    forecast_plot_filename = os.path.join(forecast_output_dir, f\"ALL_platforms_sentiment_forecast_TransformerLSTM_{forecast_months}mo.svg\")\n",
    "    try:\n",
    "        plt.savefig(forecast_plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"预测图已保存至: {forecast_plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 保存图像失败 - {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- 处理完成 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8210c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 用户配置区域 ---\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "# plt.rcParams['font.sans-serif'] = ['SimHei'] # 设置中文字体为 SimHei # 在后面有更健壮的设置\n",
    "\n",
    "# 数据文件所在的目录\n",
    "save_dir = \"E\"\n",
    "# 要处理的平台列表\n",
    "platforms = ['dy', 'xhs', 'tieba']\n",
    "# 评论数据所在的列名 (此脚本中主要用于参考，实际处理基于情感、点赞、时间列)\n",
    "comment_column = 'content'\n",
    "# 情感列名\n",
    "sentiment_column = 'sentiment'\n",
    "# 开始和结束时间 (格式: 'YYYY-MM-DD') - 请根据需要修改\n",
    "start_date_str = '2024-02-01'\n",
    "end_date_str = '2025-3-31'\n",
    "# 聚合时间频率 ('D' for Daily, 'W' for Weekly, 'M' for Monthly, '2D' for 2 days etc.)\n",
    "agg_freq = 'D'\n",
    "# 输出图像文件名\n",
    "output_plot_filename = os.path.join(save_dir, \"sentiment_evolution_plot.svg\")\n",
    "# 输出数据文件名格式 (每个平台一个文件)\n",
    "output_data_filename_template = os.path.join(save_dir, \"{platform}_sentiment_timeseries_export_1.csv\") # 使用模板\n",
    "\n",
    "# --- 核心处理逻辑 ---\n",
    "# 1. 设置中文显示字体\n",
    "try:\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei'] # 优先使用 SimHei\n",
    "except:\n",
    "    try:\n",
    "        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei'] # 备选 Microsoft YaHei\n",
    "    except:\n",
    "        print(\"警告：未找到 SimHei 或 Microsoft YaHei 字体，中文可能显示为方块。请安装中文字体。\")\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 2. 定义情感到分数的映射\n",
    "sentiment_mapping = {\n",
    "    'Very Positive': 0.6,\n",
    "    'Positive': 0.3,\n",
    "    'Neutral': 0,\n",
    "    'Negative': -0.25,\n",
    "    'Very Negative': -0.5\n",
    "}\n",
    "\n",
    "# 3. 定义平台特定的列名\n",
    "platform_configs = {\n",
    "    'dy':    {'time_col': 'create_time', 'like_col': 'like_count'},\n",
    "    'xhs':   {'time_col': 'time', 'like_col': 'like_count'},\n",
    "    'tieba': {'time_col': 'publish_time', 'like_col': None}\n",
    "}\n",
    "\n",
    "# 4. 定义开始和结束时间戳\n",
    "try:\n",
    "    start_date = pd.to_datetime(start_date_str)\n",
    "    end_date = pd.to_datetime(end_date_str)\n",
    "except ValueError:\n",
    "    print(f\"错误：开始 ({start_date_str}) 或结束日期 ({end_date_str}) 格式不正确，请使用 'YYYY-MM-DD' 格式。\")\n",
    "    exit()\n",
    "\n",
    "# 存储每个平台处理后的时间序列数据\n",
    "all_platform_sentiment = {}\n",
    "\n",
    "# 5. 循环处理每个平台的数据\n",
    "print(\"开始处理数据...\")\n",
    "for platform in platforms:\n",
    "    print(f\"\\n--- 正在处理平台: {platform} ---\") # 加个换行更清晰\n",
    "    config = platform_configs[platform]\n",
    "    time_col = config['time_col']\n",
    "    like_col = config['like_col']\n",
    "    # 直接构建文件名\n",
    "    file_path = os.path.join(save_dir, f\"{platform}_combined.csv\")\n",
    "    print(f\"读取文件: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # 读取数据，只读取需要的列以节省内存\n",
    "        use_cols = [time_col, sentiment_column]\n",
    "        if like_col:\n",
    "            use_cols.append(like_col)\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "             print(f\"警告: 文件未找到 {file_path}。跳过此平台。\")\n",
    "             continue\n",
    "\n",
    "        df = pd.read_csv(file_path, usecols=use_cols, low_memory=False)\n",
    "        print(f\"原始数据行数: {len(df)}\")\n",
    "\n",
    "        # --- 数据清洗和预处理 ---\n",
    "        # (省略了详细的清洗步骤，保持和你的代码一致)\n",
    "        # a. 处理时间列\n",
    "        print(f\"处理时间列: {time_col}\")\n",
    "        # ... (时间列清理和转换的代码) ...\n",
    "        if not pd.api.types.is_string_dtype(df[time_col]) and not pd.api.types.is_object_dtype(df[time_col]):\n",
    "             df[time_col] = df[time_col].astype(str)\n",
    "        df[time_col] = df[time_col].fillna('')\n",
    "        cleaned_time_col = df[time_col].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "        datetime_format = None\n",
    "        if platform == 'dy':\n",
    "             datetime_format = '%Y-%m-%d %H:%M:%S'\n",
    "             df[time_col] = pd.to_datetime(cleaned_time_col, format=datetime_format, errors='coerce')\n",
    "        else:\n",
    "             df[time_col] = pd.to_datetime(cleaned_time_col, errors='coerce')\n",
    "\n",
    "        invalid_time_mask = df[time_col].isna()\n",
    "        num_total_invalid = invalid_time_mask.sum()\n",
    "        if num_total_invalid > 0:\n",
    "             print(f\"时间列转换后发现 {num_total_invalid} 个无效值 (NaT)。\")\n",
    "             # (省略了详细的诊断输出代码)\n",
    "\n",
    "        original_rows = len(df)\n",
    "        df.dropna(subset=[time_col], inplace=True)\n",
    "        removed_count = original_rows - len(df)\n",
    "        if removed_count > 0:\n",
    "             print(f\"已根据 '{time_col}' 列删除 {removed_count} 行包含无效时间的数据。\")\n",
    "        print(f\"处理后剩余有效时间数据行数: {len(df)}\")\n",
    "\n",
    "        if not df.empty:\n",
    "            df[time_col] = df[time_col].dt.normalize()\n",
    "        else:\n",
    "             print(f\"警告：平台 {platform} 在时间清洗后没有剩余数据。\")\n",
    "             continue # 如果没数据了，直接跳到下一个平台\n",
    "\n",
    "\n",
    "        # c. 处理情感列\n",
    "        print(f\"处理情感列: {sentiment_column}\")\n",
    "        df['sentiment_score'] = df[sentiment_column].map(sentiment_mapping)\n",
    "        original_rows = len(df)\n",
    "        df.dropna(subset=['sentiment_score'], inplace=True)\n",
    "        print(f\"映射情感分数并行删除无效情感后行数: {len(df)} (移除了 {original_rows - len(df)} 行)\")\n",
    "        if df.empty:\n",
    "            print(f\"警告: 平台 {platform} 在映射情感分数后没有有效数据。跳过后续处理。\")\n",
    "            continue\n",
    "\n",
    "        # d. 计算加权情感分数\n",
    "        print(\"计算加权情感分数...\")\n",
    "        if like_col:\n",
    "            print(f\"使用点赞列: {like_col}\")\n",
    "            df[like_col] = pd.to_numeric(df[like_col], errors='coerce')\n",
    "            df[like_col] = df[like_col].fillna(0)\n",
    "            if (df[like_col] < 0).any():\n",
    "                df[like_col] = df[like_col].clip(lower=0)\n",
    "            df['weighted_sentiment'] = df['sentiment_score'] * (1+np.log1p(df[like_col]))\n",
    "            # 限制范围\n",
    "            df['weighted_sentiment'] = df['weighted_sentiment'].clip(lower=-0.35, upper=0.35)\n",
    "            print(f\"已将加权情感得分限制在 [-0.35, 0.35] 范围内。\")\n",
    "        else:\n",
    "            print(\"无点赞列，使用原始情感分数。\")\n",
    "            df['weighted_sentiment'] = df['sentiment_score']\n",
    "            df['weighted_sentiment'] = df['weighted_sentiment'].clip(lower=-0.35, upper=0.35)\n",
    "            print(\"已将原始情感得分(作为加权得分)限制在 [-0.35, 0.35] 范围内。\")\n",
    "\n",
    "\n",
    "        # --- 按时间聚合 ---\n",
    "        print(f\"按时间聚合 (频率: {agg_freq})...\")\n",
    "        if df.empty or 'weighted_sentiment' not in df.columns:\n",
    "             print(f\"警告: 平台 {platform} 没有有效的加权情感数据进行聚合。\")\n",
    "             continue\n",
    "\n",
    "        df.set_index(time_col, inplace=True)\n",
    "        sentiment_ts = df['weighted_sentiment'].resample(agg_freq).mean()\n",
    "\n",
    "        if sentiment_ts.empty:\n",
    "            print(f\"警告: 平台 {platform} 重采样后结果为空。\")\n",
    "            continue\n",
    "\n",
    "        print(f\"填充前 {platform} 时间序列长度: {len(sentiment_ts)}, NaN数量: {sentiment_ts.isna().sum()}\")\n",
    "        sentiment_ts.ffill(inplace=True)\n",
    "        sentiment_ts.bfill(inplace=True)\n",
    "        print(f\"填充后 {platform} 时间序列长度: {len(sentiment_ts)}, NaN数量: {sentiment_ts.isna().sum()}\")\n",
    "\n",
    "        if sentiment_ts.isna().all():\n",
    "             print(f\"警告: 平台 {platform} 的聚合时间序列在填充后仍然全部是 NaN。跳过此平台。\")\n",
    "             continue\n",
    "\n",
    "        all_platform_sentiment[platform] = sentiment_ts\n",
    "        print(f\"平台 {platform} 处理完成。聚合后时间点数量: {len(sentiment_ts)}\")\n",
    "\n",
    "        # --- 新增：导出聚合后的时间序列数据 ---\n",
    "        if not sentiment_ts.empty:\n",
    "            try:\n",
    "                # 将 Series 转换为 DataFrame 以便导出\n",
    "                # Series 的 index 是时间戳，values 是情感得分\n",
    "                df_to_export = pd.DataFrame({\n",
    "                    '时间': sentiment_ts.index, # 时间列\n",
    "                    '平均加权情感得分': sentiment_ts.values # 得分列\n",
    "                })\n",
    "                # 可选：将时间列格式化为 'YYYY-MM-DD' 字符串，更便于 Excel 查看\n",
    "                df_to_export['时间'] = df_to_export['时间'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                # 构建导出文件名\n",
    "                export_filename = output_data_filename_template.format(platform=platform)\n",
    "                # 导出到 CSV，使用 utf_8_sig 编码确保中文在 Excel 中正确显示\n",
    "                df_to_export.to_csv(export_filename, index=False, encoding='utf_8_sig')\n",
    "                print(f\"已将 {platform.upper()} 平台聚合后的情感时间序列导出至: {export_filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误: 导出平台 {platform} 的时间序列数据时发生错误: {e}\")\n",
    "        else:\n",
    "            # 这个分支理论上不会执行，因为前面有 isna().all() 的检查，但为了代码完整性保留\n",
    "            print(f\"平台 {platform} 的聚合时间序列为空，跳过导出。\")\n",
    "        # --- 导出代码结束 ---\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件未找到 {file_path}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"错误: 处理平台 {platform} 时列名错误 - {e}。请检查CSV文件中的列名是否与配置匹配。\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 处理平台 {platform} 时发生意外错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 6. 绘图\n",
    "print(\"\\n开始绘制情感演变图...\")\n",
    "if not all_platform_sentiment:\n",
    "    print(\"错误：没有成功处理任何平台的数据，无法绘图。请检查之前的处理步骤和数据。\")\n",
    "else:\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    # sns.set_style(\"whitegrid\")\n",
    "\n",
    "    colors = ['#1f77b4', '#ff7f0e', 'green']\n",
    "    color_map = {platform: colors[i] for i, platform in enumerate(all_platform_sentiment.keys())}\n",
    "\n",
    "    for platform, sentiment_ts in all_platform_sentiment.items():\n",
    "        if not sentiment_ts.empty:\n",
    "            rolling_mean_ts = sentiment_ts.rolling(window=7, center=True, min_periods=1).mean()\n",
    "            plt.plot(rolling_mean_ts.index, rolling_mean_ts.values,\n",
    "                     label=f\"{platform.upper()} 平台\",\n",
    "                     color=color_map[platform],\n",
    "                     linewidth=2,\n",
    "                     linestyle='-')\n",
    "        else:\n",
    "             print(f\"警告: 平台 {platform} 没有聚合后的数据可供绘制。\")\n",
    "\n",
    "\n",
    "    plt.xlim(start_date, end_date)\n",
    "    # plt.ylim(-0.5, 0.5) # 如果需要固定Y轴范围，取消注释\n",
    "\n",
    "    plt.title(f'各平台对国产大模型情感演变趋势', fontsize=18, pad=20)\n",
    "    plt.xlabel('日期', fontsize=14, labelpad=10)\n",
    "    plt.ylabel('平均加权情感得分', fontsize=14, labelpad=10)\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    if agg_freq == 'D':\n",
    "        total_days = (end_date - start_date).days\n",
    "        if total_days > 180:\n",
    "             plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "             plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "        elif total_days > 60:\n",
    "            plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))\n",
    "            plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "        else:\n",
    "             plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "             plt.gca().xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n",
    "    elif agg_freq == 'W':\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        plt.gca().xaxis.set_minor_locator(mdates.WeekdayLocator(interval=1))\n",
    "    else:\n",
    "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(minticks=8, maxticks=15))\n",
    "\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.8, alpha=0.7)\n",
    "    plt.legend(title=\"平台\", fontsize=12, title_fontsize=13, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "    try:\n",
    "        plt.savefig(output_plot_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"图像已保存至: {output_plot_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 保存图像失败 - {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n处理完成。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
